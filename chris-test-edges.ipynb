{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!uv sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save as test_cuda.py and run: python3 test_cuda.py\n",
    "\n",
    "import platform\n",
    "\n",
    "print(\"=== Environment ===\")\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"Python:\", platform.python_version())\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except ImportError as e:\n",
    "    print(\"\\nPyTorch is not installed or not in this Python environment.\")\n",
    "    raise SystemExit(e)\n",
    "\n",
    "print(\"\\n=== PyTorch / CUDA Info ===\")\n",
    "print(\"torch.__version__:\", torch.__version__)\n",
    "print(\"torch.version.cuda:\", torch.version.cuda)\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"torch.cuda.is_available():\", cuda_available)\n",
    "\n",
    "if not cuda_available:\n",
    "    print(\"\\nCUDA is NOT available to PyTorch in this environment.\")\n",
    "else:\n",
    "    # Number of devices\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(\"torch.cuda.device_count():\", device_count)\n",
    "\n",
    "    for i in range(device_count):\n",
    "        print(f\"  device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "    # Simple tensor test on GPU\n",
    "    try:\n",
    "        x = torch.rand(3, 3, device=\"cuda\")\n",
    "        y = torch.rand(3, 3, device=\"cuda\")\n",
    "        z = x @ y\n",
    "        print(\"\\nSuccessfully ran a matrix multiply on CUDA.\")\n",
    "        print(\"z.device:\", z.device)\n",
    "    except Exception as e:\n",
    "        print(\"\\nERROR: Allocation or compute on CUDA failed:\")\n",
    "        print(e)\n",
    "\n",
    "print(\"\\n=== Test Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the small transactions CSV (relative to this notebook).\n",
    "DATA_PATH = Path(\"dataset\") / \"HI-Small_Trans.csv\"\n",
    "\n",
    "# Load into a DataFrame\n",
    "small_trans = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Quick summary and preview\n",
    "print(f\"Loaded {len(small_trans)} rows; columns: {list(small_trans.columns)}\")\n",
    "small_trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic analysis: currencies, banks, and other summaries\n",
    "# This cell is robust to different column names: it searches for currency-like and bank-like columns\n",
    "\n",
    "# Show shape and a small sample\n",
    "rows, cols = small_trans.shape\n",
    "print(f\"Data shape: {rows} rows x {cols} columns\")\n",
    "print()\n",
    "print(\"Sample rows:\")\n",
    "display(small_trans.head())\n",
    "\n",
    "# Missing values by column (top 10)\n",
    "missing_by_col = small_trans.isnull().sum().sort_values(ascending=False).head(15)\n",
    "print(\"Top missing values by column:\")\n",
    "print(missing_by_col.to_string())\n",
    "print()\n",
    "\n",
    "# Find likely currency column(s)\n",
    "currency_candidates = [c for c in small_trans.columns if any(k in c.lower() for k in ('currency','ccy','curr'))]\n",
    "if currency_candidates:\n",
    "    cur_col = currency_candidates[0]\n",
    "    num_currencies = small_trans[cur_col].nunique(dropna=True)\n",
    "    top_currencies = small_trans[cur_col].value_counts().head(10)\n",
    "    print(f\"Found currency column: '{cur_col}' — {num_currencies} unique values\")\n",
    "    print(\"Top currencies (by count):\")\n",
    "    print(top_currencies.to_string())\n",
    "else:\n",
    "    cur_col = None\n",
    "    print(\"No currency-like column found.\")\n",
    "    print(\"Columns:\", list(small_trans.columns))\n",
    "\n",
    "print()\n",
    "# Find likely bank-related columns\n",
    "bank_candidates = [c for c in small_trans.columns if any(k in c.lower() for k in ('bank','institution','bic','iban','bankid','bank_id','bankname','bank_name'))]\n",
    "if bank_candidates:\n",
    "    # Count unique bank identifiers across candidate columns\n",
    "    unique_banks = set()\n",
    "    for c in bank_candidates:\n",
    "        unique_banks.update(small_trans[c].dropna().astype(str).unique())\n",
    "    num_unique_banks = len(unique_banks)\n",
    "    print(f\"Found bank-like columns: {bank_candidates} — approx. {num_unique_banks} unique bank identifiers (aggregated)\")\n",
    "else:\n",
    "    num_unique_banks = None\n",
    "    print(\"No bank-like columns found.\")\n",
    "\n",
    "print()\n",
    "# Other basic summaries: amount column candidates and top senders/receivers if available\n",
    "amt_candidates = [c for c in small_trans.columns if any(k in c.lower() for k in ('amount','amt','value'))]\n",
    "if amt_candidates:\n",
    "    amt_col = amt_candidates[0]\n",
    "    print(f\"Found amount column: {amt_col} — summary:\")\n",
    "    print(small_trans[amt_col].describe())\n",
    "else:\n",
    "    print(\"No amount-like column found.\")\n",
    "\n",
    "# If sender/receiver columns exist, show top participants\n",
    "party_candidates = [c for c in small_trans.columns if any(k in c.lower() for k in ('sender','receiver','originator','beneficiary','from_','to_','account'))]\n",
    "if party_candidates:\n",
    "    print()\n",
    "    print(\"Top participants in party-like columns:\")\n",
    "    for c in party_candidates:\n",
    "        print(f\"Column: {c}\")\n",
    "# Final small sample\n",
    "display(small_trans.sample(min(5, len(small_trans))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert hex account numbers to int\n",
    "hex_to_int = np.vectorize(lambda x: int(x, 16))\n",
    "\n",
    "# create adjacency lists to represent the graph\n",
    "source = hex_to_int(small_trans['Account'])\n",
    "target = hex_to_int(small_trans['Account.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Map account IDs to a compact 0..N-1 index space to avoid huge sparse IDs\n",
    "# Concatenate unique accounts from source/target and factorize\n",
    "all_accounts = np.concatenate([source, target])\n",
    "unique_accounts, inverse_idx = np.unique(all_accounts, return_inverse=True)\n",
    "num_nodes = unique_accounts.shape[0]\n",
    "# Rebuild source/target as compact indices\n",
    "source_idx = inverse_idx[:source.shape[0]]\n",
    "target_idx = inverse_idx[source.shape[0]:]\n",
    "\n",
    "# Build edge_index\n",
    "edge_index = torch.tensor(np.vstack([source_idx, target_idx]), dtype=torch.long)\n",
    "\n",
    "# Create Data object\n",
    "data = Data(edge_index=edge_index, num_nodes=num_nodes)\n",
    "print('num_nodes:', num_nodes, 'num_edges:', edge_index.size(1))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "\n",
    "# extract individual edge features\n",
    "time = pd.to_datetime(small_trans['Timestamp']).astype('int64') / 1e9\n",
    "amount_paid = small_trans['Amount Paid'].to_numpy()\n",
    "amount_received = small_trans['Amount Received'].to_numpy()\n",
    "\n",
    "# use one-hot encoding for categorical variables\n",
    "paid_enc = OneHotEncoder(sparse_output=False)\n",
    "paid_currency = paid_enc.fit_transform(small_trans['Payment Currency'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "received_enc = OneHotEncoder(sparse_output=False)\n",
    "received_currency = received_enc.fit_transform(small_trans['Receiving Currency'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "format_enc = OneHotEncoder(sparse_output=False)\n",
    "pay_format = format_enc.fit_transform(small_trans['Payment Format'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "# combine edge features into single tensor\n",
    "numeric_features = np.column_stack([time, amount_paid, amount_received])\n",
    "edge_features = torch.from_numpy(np.concatenate([numeric_features, paid_currency, received_currency, pay_format], axis=1)).float()\n",
    "\n",
    "# create edge labels\n",
    "fraud_label = torch.tensor(small_trans['Is Laundering'].to_numpy(), dtype=torch.long)\n",
    "\n",
    "# attach features and labels to PyG Data\n",
    "data.edge_attr = edge_features\n",
    "data.edge_label = fraud_label\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chronological 60/20/20 split by edge index order\n",
    "num_edges = data.edge_index.size(1)\n",
    "train_end = int(0.6 * num_edges)\n",
    "val_end = int(0.8 * num_edges)\n",
    "\n",
    "train_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_edges, dtype=torch.bool)\n",
    "train_mask[:train_end] = True\n",
    "val_mask[train_end:val_end] = True\n",
    "test_mask[val_end:] = True\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n",
    "data.test_mask = test_mask\n",
    "print('Masks set:', train_mask.sum().item(), val_mask.sum().item(), test_mask.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "\n",
    "pos_weight = 1000.0 #how much more heavily to weight positive labels \n",
    "\n",
    "epochs = 1 #number of epochs in training loop\n",
    "\n",
    "num_hid = 64 #how many hidden layers in NN\n",
    "\n",
    "learn_rate = 1e-4 #\n",
    "\n",
    "decay = 1e-4 #\n",
    "\n",
    "#default pos_weight=1000.0, since there 1000 times fewer transactions with postive labeles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PyG GNN model and edge classification training\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Ensure data object exists with edge_index, edge_attr, edge_label, and masks\n",
    "assert data is not None, 'PyG Data not constructed yet'\n",
    "num_nodes = data.num_nodes\n",
    "num_edges = data.edge_index.size(1)\n",
    "edge_feat_dim = data.edge_attr.size(1)\n",
    "\n",
    "# Create simple node features if none exist (e.g., degree or identity)\n",
    "if getattr(data, 'x', None) is None:\n",
    "    deg = torch.zeros((num_nodes, 1), dtype=torch.float)\n",
    "    deg.scatter_add_(0, data.edge_index[0].view(-1,1), torch.ones((num_edges,1)))\n",
    "    data.x = deg  # use degree as a simple node feature\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, edge_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, edge_dim)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels, edge_dim)\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class EdgeClassifier(nn.Module):\n",
    "    def __init__(self, node_hidden, edge_feat_dim, hidden=64, num_classes=2):\n",
    "        super().__init__()\n",
    "        # combine node embeddings of u and v with edge features\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(node_hidden*2 + edge_feat_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, num_classes)\n",
    "        )\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        u, v = edge_index\n",
    "        h = torch.cat([x[u], x[v], edge_attr], dim=1)\n",
    "        return self.mlp(h)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = nn.Sequential()  # placeholder to hold submodules\n",
    "gnn = GNN(in_channels=data.x.size(1), hidden_channels=64, edge_dim=edge_feat_dim).to(device)\n",
    "clf = EdgeClassifier(node_hidden=num_hid, edge_feat_dim=edge_feat_dim, hidden=num_hid*2, num_classes=2).to(device)\n",
    "\n",
    "params = list(gnn.parameters()) + list(clf.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=learn_rate, weight_decay=decay)\n",
    "criterion = nn.CrossEntropyLoss(torch.tensor([1.0, pos_weight]))\n",
    "\n",
    "# Move tensors to device\n",
    "data = data.to(device)\n",
    "train_idx = data.train_mask.nonzero(as_tuple=False).view(-1)\n",
    "val_idx = data.val_mask.nonzero(as_tuple=False).view(-1)\n",
    "test_idx = data.test_mask.nonzero(as_tuple=False).view(-1)\n",
    "\n",
    "\n",
    "def evaluate(split_idx):\n",
    "    gnn.eval(); clf.eval()\n",
    "    with torch.no_grad():\n",
    "        x = gnn(data.x, data.edge_index, data.edge_attr)\n",
    "        logits = clf(x, data.edge_index, data.edge_attr)\n",
    "        y = data.edge_label\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct = (preds[split_idx] == y[split_idx]).sum().item()\n",
    "        total = split_idx.numel()\n",
    "        return correct / max(total, 1)\n",
    "\n",
    "def binary_metrics(split_idx):\n",
    "    gnn.eval(); clf.eval()\n",
    "    with torch.no_grad():\n",
    "        x = gnn(data.x, data.edge_index, data.edge_attr)\n",
    "        logits = clf(x, data.edge_index, data.edge_attr)\n",
    "        y = data.edge_label.bool().numpy()\n",
    "        preds = logits.argmax(dim=1).bool().numpy()\n",
    "        true_pos = (preds[split_idx] & y[split_idx]).sum().item()\n",
    "        true_neg = ((~preds[split_idx]) & (~y[split_idx])).sum().item()\n",
    "        false_pos = (preds[split_idx] & (~y[split_idx])).sum().item()\n",
    "        false_neg = ((~preds[split_idx]) & y[split_idx]).sum().item()\n",
    "        return [true_pos/max((true_pos + false_pos), 1),\n",
    "                true_pos/max((true_pos + false_neg), 1),\n",
    "                false_pos/max((false_pos + true_neg), 1)]\n",
    "    \n",
    "def confusion(split_idx):\n",
    "    gnn.eval(); clf.eval()\n",
    "    with torch.no_grad():\n",
    "        x = gnn(data.x, data.edge_index, data.edge_attr)\n",
    "        logits = clf(x, data.edge_index, data.edge_attr)\n",
    "        y = data.edge_label\n",
    "        preds = logits.argmax(dim=1)\n",
    "        return confusion_matrix(y, preds)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    gnn.train(); clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    x = gnn(data.x, data.edge_index, data.edge_attr)\n",
    "    logits = clf(x, data.edge_index, data.edge_attr)\n",
    "    loss = criterion(logits[train_idx], data.edge_label[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 2 == 0 or epoch == 1:\n",
    "        train_acc = evaluate(train_idx)\n",
    "        val_acc = evaluate(val_idx)\n",
    "        bin_metrics = binary_metrics(val_idx)\n",
    "        print(f'Epoch {epoch:02d} | loss {loss.item():.4f} | train_acc {train_acc:.3f} | val_acc {val_acc:.3f} | val_precision {bin_metrics[0]:.3f} | val_recall {bin_metrics[1]:.3f} | false_pos {bin_metrics[2]:.3f}')\n",
    "\n",
    "        \n",
    "test_acc = evaluate(test_idx)\n",
    "test_recall = binary_metrics(test_idx)[1]\n",
    "false_positive = binary_metrics(test_idx)[2]\n",
    "cm = confusion(test_idx)\n",
    "print('Test accuracy:', round(test_acc, 6))\n",
    "print('Test recall', round(test_recall, 6))\n",
    "print('False positive rate', round(false_positive, 6))\n",
    "ConfusionMatrixDisplay(cm).plot() "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOv070Al0cK/8N1oI0v7dEu",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
